# ICU ‚Äì AI-Powered Environmental Awareness for the Visually Impaired

**ICU** is an assistive-technology prototype that provides visually impaired users with real-time awareness of their surroundings. A motorized camera, voice commands, and hand-gesture recognition work together to capture the environment, run AI analysis, and speak back contextual descriptions‚Äîacting as an ‚Äúextra pair of eyes‚Äù so users can navigate safely and independently.

---

## Key Features

- **Natural voice interface** ‚Äì Ask questions like ‚ÄúWhat‚Äôs in front of me?‚Äù or direct the camera with spoken commands.  
- **Gesture-based control** ‚Äì Pointing gestures are tracked with MediaPipe; the camera pans automatically to the indicated direction.  
- **Real-time scene analysis** ‚Äì Live video streams run through multimodal models (Gemini + local OpenAI Whisper) to identify objects, hazards, and text.  
- **Low-latency architecture** ‚Äì WebSockets connect phone cameras, local ML services, and cloud inference for responsive feedback.  
- **Hardware prototype**  
  - ESP-32 microcontroller driving a stepper-motor pan-tilt camera  
  - Secondary fixed camera for hand-landmark extraction  
  - Python/C++ services running on a laptop (or Raspberry Pi)  

---

## Tech Stack

| Layer        | Tools & Libraries                                   |
|--------------|-----------------------------------------------------|
| AI / ML      | vapi ‚Ä¢ Google Gemini ‚Ä¢ MediaPipe ‚Ä¢ OpenCV |
| Hardware     | ESP-32 ‚Ä¢ Stepper motors ‚Ä¢ motor drivers  |
| Backend      | Flask API ‚Ä¢ WebSockets |
| Miscellaneous| C++, Red Bull, Stethescope, Superglue üòâ |

---

## Roadmap

- **Miniaturization** ‚Äì Port to Raspberry Pi Zero 2 W or an all-mobile deployment.  
- **Expanded object ontology** ‚Äì Better recognition of medication bottles, appliance states, and low-light scenes.  
- **Smart-home integration** ‚Äì Act on findings (e.g., ‚Äúturn off the stove‚Äù) via IoT APIs.  
- **Extended user testing** ‚Äì Gather feedback from real users to refine voice NLP and gesture sensitivity.  

---

*Built at the **UC Berkeley AI Hackathon 2025** by Vyom Fadia, Jaymin Jhaveri, Katie Cheng, and Sam Mathew.*
